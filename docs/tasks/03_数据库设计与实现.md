# 任务03: 数据库设计与实现

## 任务概述

设计并实现IOPaint微信小程序的数据库结构，包括用户管理、处理记录、文件管理等核心数据表，确保数据存储的完整性和高效性。

## 技术要求

### 数据库选型
- **开发环境**: SQLite (便于开发和测试)
- **生产环境**: PostgreSQL 12+ (高并发支持)
- **ORM框架**: SQLAlchemy
- **迁移工具**: Alembic
- **连接池**: SQLAlchemy连接池管理

### 设计原则
- **数据一致性**: 外键约束和事务管理
- **查询优化**: 合理的索引设计
- **扩展性**: 支持后续功能扩展
- **安全性**: 敏感数据加密存储

## 实现步骤

### 步骤1: 数据库表设计

#### 1.1 用户表 (users)
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    openid VARCHAR(255) UNIQUE NOT NULL,
    union_id VARCHAR(255),
    nickname VARCHAR(100),
    avatar_url VARCHAR(500),
    phone VARCHAR(20),
    email VARCHAR(100),
    is_active BOOLEAN DEFAULT TRUE,
    is_premium BOOLEAN DEFAULT FALSE,
    usage_quota INTEGER DEFAULT 10,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    last_login_at TIMESTAMP WITH TIME ZONE,
    login_count INTEGER DEFAULT 0
);

-- 索引
CREATE INDEX idx_users_openid ON users(openid);
CREATE INDEX idx_users_union_id ON users(union_id) WHERE union_id IS NOT NULL;
CREATE INDEX idx_users_created_at ON users(created_at);
```

#### 1.2 处理记录表 (process_records)
```sql
CREATE TABLE process_records (
    id SERIAL PRIMARY KEY,
    user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    task_id VARCHAR(100) UNIQUE,
    original_image_url VARCHAR(500) NOT NULL,
    original_image_size INTEGER,
    result_image_url VARCHAR(500),
    result_image_size INTEGER,
    mask_data TEXT,
    model_name VARCHAR(50) NOT NULL,
    model_config JSONB,
    status VARCHAR(20) DEFAULT 'pending',
    error_message TEXT,
    progress INTEGER DEFAULT 0,
    processing_time INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE
);

-- 索引
CREATE INDEX idx_process_records_user_id ON process_records(user_id);
CREATE INDEX idx_process_records_status ON process_records(status);
CREATE INDEX idx_process_records_created_at ON process_records(created_at);
CREATE INDEX idx_process_records_task_id ON process_records(task_id);
```

#### 1.3 文件存储表 (file_storage)
```sql
CREATE TABLE file_storage (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
    file_path VARCHAR(500) NOT NULL,
    file_name VARCHAR(255) NOT NULL,
    file_type VARCHAR(50),
    file_size INTEGER,
    mime_type VARCHAR(100),
    storage_type VARCHAR(20) DEFAULT 'local',
    upload_ip VARCHAR(45),
    is_temporary BOOLEAN DEFAULT FALSE,
    expires_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- 索引
CREATE INDEX idx_file_storage_user_id ON file_storage(user_id);
CREATE INDEX idx_file_storage_file_path ON file_storage(file_path);
CREATE INDEX idx_file_storage_created_at ON file_storage(created_at);
CREATE INDEX idx_file_storage_expires_at ON file_storage(expires_at) WHERE expires_at IS NOT NULL;
```

#### 1.4 用户会话表 (user_sessions)
```sql
CREATE TABLE user_sessions (
    id SERIAL PRIMARY KEY,
    user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    session_token VARCHAR(255) UNIQUE NOT NULL,
    device_info JSONB,
    ip_address VARCHAR(45),
    user_agent TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    last_activity_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    expires_at TIMESTAMP WITH TIME ZONE NOT NULL
);

-- 索引
CREATE INDEX idx_user_sessions_user_id ON user_sessions(user_id);
CREATE INDEX idx_user_sessions_token ON user_sessions(session_token);
CREATE INDEX idx_user_sessions_expires_at ON user_sessions(expires_at);
```

#### 1.5 系统配置表 (system_configs)
```sql
CREATE TABLE system_configs (
    id SERIAL PRIMARY KEY,
    config_key VARCHAR(100) UNIQUE NOT NULL,
    config_value TEXT,
    config_type VARCHAR(20) DEFAULT 'string',
    description TEXT,
    is_public BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- 索引
CREATE INDEX idx_system_configs_key ON system_configs(config_key);
```

### 步骤2: SQLAlchemy模型实现

#### 2.1 基础模型类
```python
# iopaint/database/base.py
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import Column, Integer, DateTime
from sqlalchemy.sql import func

Base = declarative_base()

class TimestampMixin:
    """时间戳混入类"""
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

class BaseModel(Base, TimestampMixin):
    """基础模型类"""
    __abstract__ = True
    
    id = Column(Integer, primary_key=True, index=True)
```

#### 2.2 用户模型
```python
# iopaint/database/models/user.py
from sqlalchemy import Column, Integer, String, Boolean, DateTime
from sqlalchemy.orm import relationship
from ..base import BaseModel

class User(BaseModel):
    __tablename__ = "users"
    
    openid = Column(String(255), unique=True, nullable=False, index=True)
    union_id = Column(String(255), nullable=True, index=True)
    nickname = Column(String(100), nullable=True)
    avatar_url = Column(String(500), nullable=True)
    phone = Column(String(20), nullable=True)
    email = Column(String(100), nullable=True)
    is_active = Column(Boolean, default=True)
    is_premium = Column(Boolean, default=False)
    usage_quota = Column(Integer, default=10)
    last_login_at = Column(DateTime(timezone=True), nullable=True)
    login_count = Column(Integer, default=0)
    
    # 关系定义
    process_records = relationship("ProcessRecord", back_populates="user", cascade="all, delete-orphan")
    file_storage = relationship("FileStorage", back_populates="user", cascade="all, delete-orphan")
    sessions = relationship("UserSession", back_populates="user", cascade="all, delete-orphan")
```

#### 2.3 处理记录模型
```python
# iopaint/database/models/process_record.py
from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey
from sqlalchemy.orm import relationship
from sqlalchemy.dialects.postgresql import JSONB
from ..base import BaseModel

class ProcessRecord(BaseModel):
    __tablename__ = "process_records"
    
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    task_id = Column(String(100), unique=True, nullable=True)
    original_image_url = Column(String(500), nullable=False)
    original_image_size = Column(Integer, nullable=True)
    result_image_url = Column(String(500), nullable=True)
    result_image_size = Column(Integer, nullable=True)
    mask_data = Column(Text, nullable=True)
    model_name = Column(String(50), nullable=False)
    model_config = Column(JSONB, nullable=True)
    status = Column(String(20), default="pending")
    error_message = Column(Text, nullable=True)
    progress = Column(Integer, default=0)
    processing_time = Column(Integer, nullable=True)
    started_at = Column(DateTime(timezone=True), nullable=True)
    completed_at = Column(DateTime(timezone=True), nullable=True)
    
    # 关系定义
    user = relationship("User", back_populates="process_records")
```

### 步骤3: 数据库连接配置

#### 3.1 连接配置
```python
# iopaint/database/connection.py
import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import StaticPool

# 数据库配置
class DatabaseConfig:
    def __init__(self):
        self.database_url = os.getenv("DATABASE_URL", "sqlite:///./iopaint.db")
        self.echo = os.getenv("DB_ECHO", "false").lower() == "true"
        self.pool_size = int(os.getenv("DB_POOL_SIZE", "10"))
        self.max_overflow = int(os.getenv("DB_MAX_OVERFLOW", "20"))
    
    def get_engine_kwargs(self):
        kwargs = {
            "echo": self.echo,
        }
        
        if self.database_url.startswith("sqlite"):
            kwargs.update({
                "poolclass": StaticPool,
                "connect_args": {"check_same_thread": False}
            })
        else:
            kwargs.update({
                "pool_size": self.pool_size,
                "max_overflow": self.max_overflow,
                "pool_pre_ping": True,
                "pool_recycle": 3600
            })
        
        return kwargs

config = DatabaseConfig()
engine = create_engine(config.database_url, **config.get_engine_kwargs())
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db():
    """获取数据库会话"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def create_tables():
    """创建所有数据表"""
    from .models import User, ProcessRecord, FileStorage, UserSession, SystemConfig
    from .base import Base
    Base.metadata.create_all(bind=engine)

def drop_tables():
    """删除所有数据表 (仅用于开发环境)"""
    from .base import Base
    Base.metadata.drop_all(bind=engine)
```

### 步骤4: 数据库迁移

#### 4.1 Alembic配置
```python
# alembic/env.py
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
import os
import sys

# 添加项目路径
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from iopaint.database.base import Base
from iopaint.database.models import *

config = context.config

# 设置数据库URL
config.set_main_option(
    "sqlalchemy.url", 
    os.getenv("DATABASE_URL", "sqlite:///./iopaint.db")
)

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

target_metadata = Base.metadata

def run_migrations_offline():
    """离线迁移模式"""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online():
    """在线迁移模式"""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

#### 4.2 初始迁移脚本
```bash
# 初始化Alembic
alembic init alembic

# 生成初始迁移
alembic revision --autogenerate -m "Initial migration"

# 执行迁移
alembic upgrade head
```

### 步骤5: 数据访问层 (CRUD)

#### 5.1 用户CRUD操作
```python
# iopaint/database/crud/user.py
from sqlalchemy.orm import Session
from typing import Optional
from ..models.user import User
from ...schemas.user import UserCreate, UserUpdate

class UserCRUD:
    def get_by_id(self, db: Session, user_id: int) -> Optional[User]:
        return db.query(User).filter(User.id == user_id).first()
    
    def get_by_openid(self, db: Session, openid: str) -> Optional[User]:
        return db.query(User).filter(User.openid == openid).first()
    
    def create(self, db: Session, user_data: UserCreate) -> User:
        user = User(**user_data.dict())
        db.add(user)
        db.commit()
        db.refresh(user)
        return user
    
    def update(self, db: Session, user: User, user_data: UserUpdate) -> User:
        for field, value in user_data.dict(exclude_unset=True).items():
            setattr(user, field, value)
        db.commit()
        db.refresh(user)
        return user
    
    def update_login_info(self, db: Session, user: User) -> User:
        from datetime import datetime
        user.login_count += 1
        user.last_login_at = datetime.utcnow()
        db.commit()
        db.refresh(user)
        return user

user_crud = UserCRUD()
```

### 步骤6: 数据库工具和维护

#### 6.1 数据库工具脚本
```python
# scripts/db_tools.py
import asyncio
from sqlalchemy.orm import Session
from iopaint.database.connection import SessionLocal, create_tables, drop_tables
from iopaint.database.models import User, ProcessRecord

class DatabaseManager:
    def __init__(self):
        self.db = SessionLocal()
    
    def reset_database(self):
        """重置数据库"""
        drop_tables()
        create_tables()
        print("Database reset completed")
    
    def seed_data(self):
        """填充测试数据"""
        # 创建测试用户
        test_user = User(
            openid="test_openid_123",
            nickname="测试用户",
            avatar_url="https://example.com/avatar.jpg"
        )
        self.db.add(test_user)
        self.db.commit()
        print("Test data seeded")
    
    def cleanup_expired_files(self):
        """清理过期文件"""
        from datetime import datetime
        from ..models.file_storage import FileStorage
        
        expired_files = self.db.query(FileStorage).filter(
            FileStorage.expires_at < datetime.utcnow()
        ).all()
        
        for file in expired_files:
            # 删除物理文件
            import os
            if os.path.exists(file.file_path):
                os.remove(file.file_path)
            
            # 删除数据库记录
            self.db.delete(file)
        
        self.db.commit()
        print(f"Cleaned up {len(expired_files)} expired files")

if __name__ == "__main__":
    db_manager = DatabaseManager()
    
    import sys
    if len(sys.argv) > 1:
        if sys.argv[1] == "reset":
            db_manager.reset_database()
        elif sys.argv[1] == "seed":
            db_manager.seed_data()
        elif sys.argv[1] == "cleanup":
            db_manager.cleanup_expired_files()
    else:
        print("Usage: python db_tools.py [reset|seed|cleanup]")
```

## 验收标准

### 数据库结构验收
- [ ] 所有数据表创建成功
- [ ] 外键约束正确建立
- [ ] 索引创建完整
- [ ] 数据类型定义正确

### 功能验收
- [ ] CRUD操作正常执行
- [ ] 数据库连接池正常工作
- [ ] 事务处理正确
- [ ] 迁移脚本可以正常执行

### 性能验收
- [ ] 查询性能测试通过
- [ ] 并发访问测试通过
- [ ] 索引优化效果验证
- [ ] 连接池配置合理

## 时间估算

- **总预估时间**: 3-4天
- **表结构设计**: 1天
- **模型实现**: 1天
- **迁移配置**: 1天
- **CRUD开发和测试**: 1天

## 依赖任务

- 任务01: 开发环境搭建

## 风险点

1. **数据迁移**: 生产环境数据迁移风险
2. **性能问题**: 大数据量下的查询性能
3. **数据一致性**: 并发操作的数据一致性
4. **备份恢复**: 数据备份和恢复策略

## 解决方案

1. **迁移测试**: 在测试环境充分测试迁移脚本
2. **性能监控**: 建立数据库性能监控
3. **事务管理**: 合理使用数据库事务
4. **备份策略**: 建立自动备份机制

## 参考资料

- [SQLAlchemy官方文档](https://docs.sqlalchemy.org/)
- [Alembic迁移工具文档](https://alembic.sqlalchemy.org/)
- [PostgreSQL优化指南](https://www.postgresql.org/docs/current/performance-tips.html)

## 完成标志

数据库实现完成后，应该能够：
1. 正常创建和连接数据库
2. 执行所有CRUD操作
3. 数据库迁移脚本正常工作
4. 通过性能和压力测试
5. 数据备份和恢复功能正常 